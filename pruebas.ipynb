{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Co-occurrence methods needed to obtain the co-occurrence representation\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def ini_cooc_filter(channels, cooc_r):\n",
    "    \"\"\" Method to obtain a co-occurrence filter\n",
    "    given the number of channels and r\n",
    "    \"\"\"\n",
    "    cooc_w = cooc_r * 2 + 1\n",
    "    cooc_filter = np.ones((channels, channels, 1, 1))/math.pow(cooc_w, 2)\n",
    "    for i in range(channels):\n",
    "        cooc_filter[i, i, :, :] = 1e-10\n",
    "    cooc_filter = torch.FloatTensor(cooc_filter).cuda()\n",
    "    cooc_filter = cooc_filter.repeat(1, 1, cooc_w, cooc_w)\n",
    "    return cooc_filter\n",
    "\n",
    "def calc_spatial_cooc(tensor, cooc_filter, cooc_r):\n",
    "    \"\"\" Method to obtain the co-occurrence representation\n",
    "    given an activation tensor, co-occurrence filter and the size r\n",
    "    \"\"\"\n",
    "    act_m = tensor > torch.mean(tensor)\n",
    "    act = tensor * act_m.float()\n",
    "    cooc_map = torch.nn.functional.conv2d(act, cooc_filter, padding=cooc_r)\n",
    "\n",
    "    cooc_map = cooc_map / (tensor.shape[1] - 1)\n",
    "    cooc_map = cooc_map * act_m.float()\n",
    "    return cooc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments\n",
    "\n",
    "* tensor: activation 4D tensor with shape: (batch_size, rows, cols, channels). //En teoria es (batch, channels, rows, cols)\n",
    "\n",
    "* cooc_filter: activation 4D tensor with shape: (channels, channels, 2 * cooc_r + 1, 2 * cooc_r + 1). This cooc_filter can be initialized using the function called ini_cooc_filter(channels, cooc_r) \n",
    "also in cooccurrences.py.\n",
    "\n",
    "* cooc_r: is the radious considered for the co-occurrence window\n",
    "\n",
    "Output\n",
    "\n",
    "* cooc_tensor: activation 4D tensor with same shape than the input activation 4D tensor: (batch_size, rows, cols, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 100\n",
    "cooc_r = 2\n",
    "\n",
    "cooc_w = cooc_r * 2 + 1\n",
    "cooc_filter = np.ones((channels, channels, 1, 1))#/math.pow(cooc_w, 2)\n",
    "for i in range(channels):\n",
    "    cooc_filter[i, i, :, :] = 1e-10\n",
    "cooc_filter = torch.FloatTensor(cooc_filter).cuda()\n",
    "cooc_filter = cooc_filter.repeat(1, 1, cooc_w, cooc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooc_mapa = torch.rand((1,5,10,10), requires_grad=True).cuda()\n",
    "\n",
    "cooc_mapb = torch.rand((1,5,15,15), requires_grad= True).cuda()\n",
    "\n",
    "\n",
    "cooc_map1 = torch.mean(cooc_mapa, dim = [0,1])\n",
    "\n",
    "cooc_map1 = torch.mean(cooc_mapa, dim = [0,1])\n",
    "\n",
    "cooc_map1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49.5607, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bbb =  torch.sum(torch.nn.functional.normalize(cooc_mapa, eps = 0, dim = [0,1], p=0))\n",
    "print(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "aa = torch.sum(torch.div(cooc_mapa, torch.sum(cooc_mapa)))\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 3\n",
    "\n",
    "t = 0.8 #threshold o umbral\n",
    "b,c,h,w = cooc_mapa.size()\n",
    "\n",
    "aux = torch.zeros(cooc_mapa.size(), dtype=torch.bool)\n",
    "\n",
    "for (b1,c1,h1,w1) in range((b,c,h,w)):\n",
    "    for b2,c2,h2,w2 in cooc_mapa.size():\n",
    "        if abs(h1-h2) > radius and abs(w1-w2) > radius and cooc_mapa[b2,c2,h2,w2] > t and cooc_mapa[b1,c1,h1,w1] > t:\n",
    "            aux[b1,c1,h1,w1] = 1\n",
    "        else:\n",
    "            aux[b1,c1,h1,w1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 3\n",
    "\n",
    "t = 0.1 #threshold o umbral\n",
    "b,c,h,w = cooc_mapa.size()\n",
    "\n",
    "aux = torch.zeros(cooc_mapa.size(), dtype=torch.bool)\n",
    "\n",
    "for b1 in range(b):\n",
    "    for c1 in range(c):\n",
    "        for h1 in range(h):\n",
    "            for w1 in range(w):\n",
    "                for b2 in range(b):\n",
    "                    for c2 in range(c):\n",
    "                        for h2 in range(h):\n",
    "                            for w2 in range(w):\n",
    "                                if abs(h1-h2) > radius and abs(w1-w2) > radius and cooc_mapa[b2,c2,h2,w2] > t and cooc_mapa[b1,c1,h1,w1] > t:\n",
    "                                    aux[b1,c1,h1,w1] = 1\n",
    "                                else:\n",
    "                                    aux[b1,c1,h1,w1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cooc_mapa > 0.99) #torch.mean(cooc_mapa))\n",
    "\n",
    "aux = cooc_mapa > 0.2\n",
    "\n",
    "cooc_mapc = cooc_mapa*aux\n",
    "\n",
    "print(cooc_mapc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = cooc_mapa\n",
    "\n",
    "B, C, H, W = input.size()\n",
    "x_flat = input.view(C, H * W)\n",
    "grama = torch.mm(x_flat, x_flat.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = cooc_mapb\n",
    "\n",
    "B, C, H, W = input.size()\n",
    "x_flat = input.view(C, H * W)\n",
    "gramb = torch.mm(x_flat, x_flat.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grama.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "out = loss(grama, gramb)\n",
    "#out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_filter = ini_cooc_filter(5, 1)\n",
    "cooc_map_fixa = calc_spatial_cooc(cooc_mapa, cooc_filter, 1)\n",
    "\n",
    "cooc_map_fixb = calc_spatial_cooc(cooc_mapb, cooc_filter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss(cooc_map_fixa, cooc_map_fixb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "COOC_FILTER_PATH = \"./data/weights_cooc_44_best_model_8192_ft.npy\"\n",
    "\n",
    "# Using VGG torch features from https://github.com/filipradenovic/cnnimageretrieval-pytorch\n",
    "VGG16 = \"http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/imagenet/imagenet-caffe-vgg16-features-d369c8e.pth\"\n",
    "\n",
    "def plot_figures(img, act_m, cooc_m_f, cooc_m_l):\n",
    "    \"\"\" Method to plot the example figures\n",
    "    \"\"\"\n",
    "\n",
    "    _, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    ax1.imshow(img)\n",
    "    ax1.set(title='Image')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax1.get_yticklabels(), visible=False)\n",
    "    ax1.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "    ax2.imshow(np.sum(act_m, axis=0))\n",
    "    ax2.set(title='Activations')\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    ax2.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "    ax3.imshow(np.sum(cooc_m_f, axis=0))\n",
    "    ax3.set(title='Direct co-occurrences')\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax3.get_yticklabels(), visible=False)\n",
    "    ax3.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "    ax4.imshow(np.sum(cooc_m_l, axis=0))\n",
    "    ax4.set(title='Learned co-occurrences')\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax4.get_yticklabels(), visible=False)\n",
    "    ax4.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" Main \"\"\"\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--test_image', dest='test_image', type=str,\n",
    "                        default=\"./data/all_souls_000117.jpg\", help='path to test image')\n",
    "    parser.add_argument('--cooc_r', dest='cooc_r', type=int, default=4, help='cooc r size')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load image and activation map\n",
    "    image = Image.open(args.test_image)\n",
    "    image = np.array(image)\n",
    "    net_image = np.moveaxis(np.array(image), 2, 0)\n",
    "\n",
    "    VGG16_net = models.vgg16(pretrained=False).features\n",
    "\n",
    "    model_dir = os.path.join(os.path.join(os.getcwd(), 'data'), 'networks')\n",
    "    VGG16_net.load_state_dict(model_zoo.load_url(VGG16, model_dir=model_dir))\n",
    "    VGG16_net.cuda()\n",
    "    VGG16_net.eval()\n",
    "\n",
    "    net_input = torch.FloatTensor(np.expand_dims(net_image, axis=0)).cuda()\n",
    "    act_map_t = VGG16_net(net_input)\n",
    "    depth = act_map_t.shape[1]\n",
    "\n",
    "    # Co-occurrences calcultation\n",
    "    # Direct cooc_filter\n",
    "    cooc_filter = ini_cooc_filter(depth, args.cooc_r)\n",
    "    cooc_map_fix = calc_spatial_cooc(act_map_t, cooc_filter, args.cooc_r)\n",
    "    cooc_map_fix = cooc_map_fix.cpu().data.squeeze().numpy()\n",
    "\n",
    "    # Co-occurrences calcultation\n",
    "    # Learned cooc_filter trained previously\n",
    "    cooc_filter = np.load(COOC_FILTER_PATH)\n",
    "    cooc_filter = torch.FloatTensor(cooc_filter).cuda()\n",
    "    cooc_map_learned = calc_spatial_cooc(act_map_t, cooc_filter, 4)\n",
    "    cooc_map_learned = cooc_map_learned.cpu().data.squeeze().numpy()\n",
    "\n",
    "    # Graphic representation\n",
    "    act_map = act_map_t.cpu().data.squeeze().numpy()\n",
    "    plot_figures(image, act_map, cooc_map_fix, cooc_map_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7725fad870e428f1e3698a5c0a0de2a7916bab2786fc0bac9119a8ae59d8d68"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('deep-learning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
