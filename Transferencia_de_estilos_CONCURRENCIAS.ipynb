{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sapotomate/Style-transfer-with-coocurrence-tensors/blob/main/Transferencia_de_estilos_CONCURRENCIAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCnpGwYO0nNj"
      },
      "source": [
        "# Red de transferencia de estilos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvDLSXwR0nNl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o93k7mb0nNn"
      },
      "source": [
        "Generamos nuestra clase modelo de red neuronal (en este caso creamos ambas partes de obtencion de características y de clasificacion, aunque solo usaremos la primera) junto con diccionarios de nombres de las capas, los cuales con ayuda de la funcion build_sequential() creamos el esqueleto y con el .pth la poblamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njVUoya00nNo"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, features, num_classes=1000):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "channel_list = {\n",
        "'VGG-19': [64, 64, 'P', 128, 128, 'P', 256, 256, 256, 256, 'P', 512, 512, 512, 512, 'P', 512, 512, 512, 512, 'P'],\n",
        "}\n",
        "\n",
        "vgg19_dict = {\n",
        "'C': ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv3_4', 'conv4_1', 'conv4_2', 'conv4_3', 'conv4_4', 'conv5_1', 'conv5_2', 'conv5_3', 'conv5_4'],\n",
        "'R': ['relu1_1', 'relu1_2', 'relu2_1', 'relu2_2', 'relu3_1', 'relu3_2', 'relu3_3', 'relu3_4', 'relu4_1', 'relu4_2', 'relu4_3', 'relu4_4', 'relu5_1', 'relu5_2', 'relu5_3', 'relu5_4'],\n",
        "'P': ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
        "}\n",
        "\n",
        "def buildSequential(channel_list, pooling):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    if pooling == 'max':\n",
        "        pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    elif pooling == 'avg':\n",
        "        pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    else:\n",
        "        raise ValueError(\"Parametro de pooling incorrecto\")\n",
        "    for c in channel_list:\n",
        "        if c == 'P':\n",
        "            layers += [pool2d]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = c\n",
        "    return nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcfss2VK0nNp"
      },
      "source": [
        "Creamos una instancia de la red, junto con algunas opciones para facilitar la ejecución en un pc local:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIrTa6dl0nNq"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True #cambiar a false en caso de fallo\n",
        "dtype = torch.cuda.FloatTensor\n",
        "backward_device = \"cuda:0\"\n",
        "device = \"cuda\"\n",
        "\n",
        "pooling = 'max'\n",
        "\n",
        "cnn, layerList = VGG(buildSequential(channel_list['VGG-19'], pooling)), vgg19_dict\n",
        "cnn.load_state_dict(torch.load('vgg19-d01eb7cb.pth'))\n",
        "cnn = cnn.cuda()\n",
        "cnn = cnn.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ZtedpB0nNq"
      },
      "source": [
        "Ahora vamos a incluir las imagenes que vamos a utilizar en la red y su tamaño, para preprocesarlas posteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rBTecBX0nNr"
      },
      "outputs": [],
      "source": [
        "input_content = 'golden_gate.jpg'\n",
        "input_style = 'starry_night.jpg'\n",
        "\n",
        "#image_size = 512\n",
        "image_size = tuple([380, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WwiCy3l0nNr"
      },
      "source": [
        "Vamos a construir dos funciones compuestas por tuberías (sucesiones de operaciones) para preparar las imágenes para entrar en la red (normalización, transofrmación a tensor, rangos 0..255, etc.) y otra para revertir esa preparación una vez ha salido de la red y devolverla a su formato original (revertir de tensor a imagen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVuDxLfX0nNs"
      },
      "outputs": [],
      "source": [
        "def preprocesamiento(image_name, image_size):\n",
        "    image = Image.open(image_name).convert('RGB')\n",
        "    if type(image_size) is not tuple:\n",
        "        image_size = tuple([int((float(image_size) / max(image.size))*x) for x in (image.height, image.width)])\n",
        "    Loader = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n",
        "    rgb2bgr = transforms.Compose([transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])])]) #pasar de RGB a BGR\n",
        "    Normalize = transforms.Compose([transforms.Normalize(mean=[103.939, 116.779, 123.68], std=[1,1,1])]) #restar la media dada de la base de datos imagenet\n",
        "    tensor = Normalize(rgb2bgr(Loader(image) * 255)).unsqueeze(0)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def postprocesamiento(output_tensor):\n",
        "    Normalize = transforms.Compose([transforms.Normalize(mean=[-103.939, -116.779, -123.68], std=[1,1,1])]) #sumar la media dada de la base de datos imagenet\n",
        "    bgr2rgb = transforms.Compose([transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])])])   #pasar de BGR a RGB\n",
        "    output_tensor = bgr2rgb(Normalize(output_tensor.squeeze(0).cpu())) / 255\n",
        "    output_tensor.clamp_(0, 1)\n",
        "    Image2PIL = transforms.ToPILImage()\n",
        "    image = Image2PIL(output_tensor.cpu())\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6zudNG90nNs"
      },
      "source": [
        "Finalmente vamos a cargar las imágenes de entrada y a crear la imagen target, la imagen de salida con la mezcla de estilo y contenido, para ya centrarnos en la red\n",
        "\n",
        "* Podemos crear la imagen target de dos maneras, que empiece como un tensor aleatorio o que parta de una imagen dada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HbYYptE0nNt"
      },
      "outputs": [],
      "source": [
        "#content_image = Image.open(input_content)\n",
        "#style_image = Image.open(input_style).convert('RGB')\n",
        "\n",
        "content_image = preprocesamiento(input_content,image_size)\n",
        "style_image = preprocesamiento(input_style,image_size)\n",
        "\n",
        "content_image = content_image.cuda()\n",
        "style_image = style_image.cuda()\n",
        "\n",
        "#Variable(content_image.unsqueeze(0).cuda())\n",
        "#Variable(style_image.unsqueeze(0).cuda())\n",
        "\n",
        "\n",
        "b, c, h, w = content_image.size()\n",
        "\n",
        "target_image = torch.randn(c, h, w).mul(0.001).unsqueeze(0).type(dtype).cuda() #imagen aleatoria\n",
        "#target_image = content_image.clone().cuda() #partir de la imagen de contenido de entrada (puede llegar a dar mejores resultados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-8zPUxS0nNt",
        "outputId": "953f96a4-8217-4465-82af-3766b3aac59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 380, 512])\n",
            "torch.Size([1, 3, 380, 512])\n",
            "torch.Size([1, 3, 380, 512])\n"
          ]
        }
      ],
      "source": [
        "print(target_image.size())\n",
        "print(content_image.size())\n",
        "print(style_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9nVMrxT0nNv"
      },
      "source": [
        "## Módulos para agregar a nuestra red\n",
        "\n",
        "A continuacion vamos a definir los módulos que necesitamos para ejecutar la transferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYDkNIFa0nNv"
      },
      "source": [
        "###  Módulo para calcular la Matriz de Gram\n",
        "\n",
        "La salida de cada capa convolucional es un Tensor con dimensiones b, d, h, w:\n",
        "- b: batch_size (cantidad de muestras que se pasan a la red de una, en nuestro caso usualmente será 1)\n",
        "- d: depth (profundidad del Tensor) // puede tratarse tambien como la cantidad de canales (c: channels)\n",
        "- h: height (altura del Tensor)\n",
        "- w: width (anchura del Tensor)\n",
        "\n",
        "Para calcular la matriz de Gram, tenemos que conseguir esos parámetros de nuestro Tensor, y crear uno nuevo pero de diferente tamaño, agrupandose de la siguiente manera:\n",
        "\n",
        "Tensor de tamaño (b x d, h x w)\n",
        "\n",
        "Ahora hay que realizar el producto escalar de los mapas de características en sus dos dimensiones, para cada capa convolucional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAuwkNTr0nNv"
      },
      "outputs": [],
      "source": [
        "class GramMatrix(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, C, H, W = input.size()\n",
        "        x_flat = input.view(C, H * W)\n",
        "        return torch.mm(x_flat, x_flat.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHyyxbSJ0nNw"
      },
      "source": [
        "### Módulo para aplicar un filtro de concurrencias\n",
        "\n",
        "Vamos a sustituir el uso de la matriz de Gram con el cálculo de tensores de concurrencias para intentar mejorar el resultado final de la imagen, en el que vamos a dar un radio de ventana deslizante y el numero de canales de los que se van a calcular las concurrencias entre sí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR0GXf-e0nNx"
      },
      "outputs": [],
      "source": [
        "class CoOccurrenceFilter(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, cooc_r, out_shape):\n",
        "        super(CoOccurrenceFilter, self).__init__()\n",
        "        self.radius = cooc_r\n",
        "        self.channels = channels\n",
        "        cooc_w = self.radius * 2 + 1\n",
        "        cooc_filter = np.ones((channels, channels, 1, 1))#/math.pow(cooc_w, 2)\n",
        "        for i in range(channels):\n",
        "            cooc_filter[i, i, :, :] = 0 #1e-10\n",
        "        cooc_filter = torch.FloatTensor(cooc_filter).cuda()\n",
        "        cooc_filter = cooc_filter.repeat(1, 1, cooc_w, cooc_w)\n",
        "        self.cooc_filter = cooc_filter\n",
        "        self.shape = out_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        act_m = input > torch.mean(input)\n",
        "        act = input * act_m.float()\n",
        "\n",
        "        #act = input*aux.float()\n",
        "        cooc_map = torch.nn.functional.conv2d(act, self.cooc_filter, padding = self.radius)\n",
        "\n",
        "        cooc_map = cooc_map / (input.shape[1] - 1)\n",
        "        cooc_map = cooc_map * act_m.float() #aux.float()\n",
        "\n",
        "        if self.shape == \"raw\":\n",
        "            B, C, H, W = cooc_map.size()\n",
        "            cooc_gram_flat = cooc_map.view(C, H * W)\n",
        "            return  torch.mm(cooc_gram_flat, cooc_gram_flat.t())\n",
        "            ##return cooc_map\n",
        "        elif self.shape == \"vector\":\n",
        "            cooc_map = torch.mean(cooc_map, dim = [2,3])\n",
        "            return cooc_map\n",
        "        elif self.shape == \"matrix\":\n",
        "            cooc_map = torch.mean(cooc_map, dim = [0,1])\n",
        "            cooc_map = torch.div(cooc_map, torch.sum(cooc_map)) ##normalización de la matriz\n",
        "            return cooc_map\n",
        "        else:\n",
        "            ##Devolvemos el tensor por defecto original\n",
        "            return cooc_map\n",
        "        #return cooc_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4pHNDs0nNx"
      },
      "source": [
        "### Módulo para calcular el error en el contenido de la imagen\n",
        "\n",
        "Para calcular el error del contenido se calcula la distancia cuadrática media entre la imagen contenido y la imagen objetivo (ruido aleatorio u imagen predefinida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO0G36yC0nNy"
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, strength):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.strength = strength\n",
        "        self.crit = nn.MSELoss()\n",
        "        self.mode = 'None'\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.mode == 'loss':\n",
        "            loss = self.crit(input, self.target)\n",
        "            self.loss = loss * self.strength\n",
        "        elif self.mode == 'capture':\n",
        "            self.target = input.detach()\n",
        "        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nau9laK-0nNy"
      },
      "source": [
        "### Módulo para calcular el error en el estilo de la imagen\n",
        "\n",
        "Para calcular el error de la imagen primero se tiene que calcular el error cuadratico medio de cada capa de la matriz de Gram con la imagen objetivo e ir sumando esos errores multiplicados por el peso asociado a cada capa convolucional de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2F3gGLz0nNz"
      },
      "outputs": [],
      "source": [
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, strength):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = torch.Tensor()\n",
        "        self.strength = strength\n",
        "        self.gram = GramMatrix()        \n",
        "        self.crit = nn.MSELoss()\n",
        "        self.mode = 'None'\n",
        "        self.cooc = True #True si quieres usar concurrencias, false si qieres Gram\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.cooc:\n",
        "            radius = 3\n",
        "            shape = \"raw\"  #raw, matrix o vector\n",
        "            _, channels, _, _ = input.size()\n",
        "            aux = CoOccurrenceFilter(channels, radius, shape)\n",
        "            self.G = aux(input)\n",
        "        else:\n",
        "            self.G = self.gram(input)\n",
        "        self.G = self.G.div(input.nelement())\n",
        "        #print(self.G)\n",
        "        if self.mode == 'capture':\n",
        "            self.target = self.G.detach()\n",
        "        elif self.mode == 'loss':\n",
        "            loss = self.crit(self.G, self.target)\n",
        "            self.loss = self.strength * loss\n",
        "        return input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMJTqhmt0nNz"
      },
      "source": [
        "### Capas de la red neuronal a utilizar\n",
        "\n",
        "Además de pesos generalizados para el contenido y el estilo, seleccionamos también las capas de la red que utilizaremos para calcular las pérdidas segun el artículo\n",
        "\n",
        "Por otra parte, vamos a asignar un peso de 1/5 a cada capa de la que se va a extraer características del estilo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvdRyV_x0nNz"
      },
      "outputs": [],
      "source": [
        "content_layers = ['relu4_2']\n",
        "content_weight = 1e0\n",
        "style_layers = ['relu1_1','relu2_1','relu3_1','relu4_1','relu5_1']\n",
        "style_weight = 1e2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFdV30HC0nN0"
      },
      "source": [
        "Ahora vamos a introducir en la red los módulos de cálculo de error de contenido y estilo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWuAAvBz0nN0",
        "outputId": "b6058203-ab48-4ddd-9059-fd72af8c0f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agregada capa de estilo ReLu 2: relu1_1\n",
            "Agregada capa de pooling 5: pool1\n",
            "Agregada capa de estilo ReLu 7: relu2_1\n",
            "Agregada capa de pooling 10: pool2\n",
            "Agregada capa de estilo ReLu 12: relu3_1\n",
            "Agregada capa de pooling 19: pool3\n",
            "Agregada capa de estilo ReLu 21: relu4_1\n",
            "Agregada capa de contenido ReLU 23: relu4_2\n",
            "Agregada capa de pooling 28: pool4\n",
            "Agregada capa de estilo ReLu 30: relu5_1\n"
          ]
        }
      ],
      "source": [
        "cnn = copy.deepcopy(cnn)\n",
        "content_losses, style_losses = [], []\n",
        "next_content_idx, next_style_idx = 1, 1\n",
        "net = nn.Sequential().cuda()\n",
        "c, r, p = 0, 0, 0\n",
        "\n",
        "layerList = vgg19_dict\n",
        "\n",
        "for i, layer in enumerate(list(cnn), 1):\n",
        "    if next_content_idx <= len(content_layers) or next_style_idx <= len(style_layers):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            net.add_module(str(len(net)), layer)\n",
        "            if layerList['C'][c] in content_layers:\n",
        "                print(\"Agregada capa de contenido convolucional \" + str(i) + \": \" + str(layerList['C'][c]))\n",
        "                loss_module = ContentLoss(content_weight)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                content_losses.append(loss_module)\n",
        "            if layerList['C'][c] in style_layers:\n",
        "                print(\"Agregada capa de estilo convolucional \" + str(i) + \": \" + str(layerList['C'][c]))\n",
        "                loss_module = StyleLoss(style_weight)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                style_losses.append(loss_module)\n",
        "            c+=1\n",
        "\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            net.add_module(str(len(net)), layer)\n",
        "\n",
        "            if layerList['R'][r] in content_layers:\n",
        "                print(\"Agregada capa de contenido ReLU \" + str(i) + \": \" + str(layerList['R'][r]))\n",
        "                loss_module = ContentLoss(content_weight)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                content_losses.append(loss_module)\n",
        "                next_content_idx += 1\n",
        "\n",
        "            if layerList['R'][r] in style_layers:\n",
        "                print(\"Agregada capa de estilo ReLu \" + str(i) + \": \" + str(layerList['R'][r]))\n",
        "                loss_module = StyleLoss(style_weight)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                style_losses.append(loss_module)\n",
        "                next_style_idx += 1\n",
        "            r+=1\n",
        "\n",
        "        if isinstance(layer, nn.MaxPool2d) or isinstance(layer, nn.AvgPool2d):\n",
        "            print(\"Agregada capa de pooling \" + str(i) + \": \" + str(layerList['P'][p]))\n",
        "            net.add_module(str(len(net)), layer)\n",
        "            p+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75NCDRS0nN1"
      },
      "source": [
        "Vamos a ver lo que hemos conseguido con esto.\n",
        "* En primer lugar hemos obtenido una nueva red que intercala la extracción de características con la obtencion de errores y pooling\n",
        "    * Podemos observar que a la hora de avanzar en la red automáticamente calculará los errores tras recibir las características obtenidas por la aplicación del filtro correspondiente de la capa convolucional y su función de activación ReLU\n",
        "    * Los módulos del calculo de error se introducen después de detectar una de las capas de pérdida para calcular el error que hemos definido anteriormente\n",
        "\n",
        "* En segundo lugar hemos obtenido un array \"style_losses\" el cual ha guardado la sucesion de los 5 módulos tras las capas que hemos indicado anteriormente para calcular el error del estilo\n",
        "    * Como se indica en el artículo, una parte del cálculo del error del estilo es calcular el error por separado de cada capa haciendo uso de las matrices de Gram tanto de la imagen objetivo como de la de estilo y mas tarde sumar todas ellas con sus respectivos pesos por capa\n",
        "\n",
        "* En tercer lugar hemos obtenido un array \"content_losses\" con el mismo principio que el anterior, solo que en este simplemente se va a calcular el error cuadrático medio entre las características del contenido y las de la imagen objetivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k5sKgV20nN1",
        "outputId": "42824e1f-fa89-49e4-d476-a249eb2f4262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Red obtenida: \n",
            " Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): StyleLoss(\n",
            "    (gram): GramMatrix()\n",
            "    (crit): MSELoss()\n",
            "  )\n",
            "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU(inplace=True)\n",
            "  (8): StyleLoss(\n",
            "    (gram): GramMatrix()\n",
            "    (crit): MSELoss()\n",
            "  )\n",
            "  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (10): ReLU(inplace=True)\n",
            "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): StyleLoss(\n",
            "    (gram): GramMatrix()\n",
            "    (crit): MSELoss()\n",
            "  )\n",
            "  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (16): ReLU(inplace=True)\n",
            "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (22): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (23): ReLU(inplace=True)\n",
            "  (24): StyleLoss(\n",
            "    (gram): GramMatrix()\n",
            "    (crit): MSELoss()\n",
            "  )\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): ContentLoss(\n",
            "    (crit): MSELoss()\n",
            "  )\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (31): ReLU(inplace=True)\n",
            "  (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (34): ReLU(inplace=True)\n",
            "  (35): StyleLoss(\n",
            "    (gram): GramMatrix()\n",
            "    (crit): MSELoss()\n",
            "  )\n",
            ")\n",
            "Calculos de error de estilo\n",
            " [StyleLoss(\n",
            "  (gram): GramMatrix()\n",
            "  (crit): MSELoss()\n",
            "), StyleLoss(\n",
            "  (gram): GramMatrix()\n",
            "  (crit): MSELoss()\n",
            "), StyleLoss(\n",
            "  (gram): GramMatrix()\n",
            "  (crit): MSELoss()\n",
            "), StyleLoss(\n",
            "  (gram): GramMatrix()\n",
            "  (crit): MSELoss()\n",
            "), StyleLoss(\n",
            "  (gram): GramMatrix()\n",
            "  (crit): MSELoss()\n",
            ")]\n",
            "Calculos de error de contenido\n",
            " [ContentLoss(\n",
            "  (crit): MSELoss()\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "#red obtenida\n",
        "print(\"Red obtenida: \\n\", net)\n",
        "print(\"Calculos de error de estilo\\n\",style_losses)\n",
        "print(\"Calculos de error de contenido\\n\",content_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x8cwGJD0nN2"
      },
      "source": [
        "### Captura de características del contenido\n",
        "\n",
        "Como podemos observar en la implementación del modulo de calculo de las perdidas en el contenido, éste tiene un atributo 'mode', el cual va a dictar si va a capturar (capture) o a calcular el error (loss). Pues bien ahora vamos a iterar sobre el array de módulos 'content_losses' que hemos obtenido anteriormente e indicar el modo que queremos en ellos.\n",
        "\n",
        "Una vez indicado el modo pasamos a ejecutar nuestra red obtenida anteriormente con nuestra imagen de contenido. (Nótese que utilizamos solo los modulos de error de contenido ya que el resto por defecto tienen el atributo a none)\n",
        "\n",
        "Finalmente vamos a devolver los modos del array de modulos a su estado original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDFnOOeM0nN2"
      },
      "outputs": [],
      "source": [
        "for i in content_losses:\n",
        "    i.mode = 'capture'\n",
        "\n",
        "net(content_image)\n",
        "\n",
        "for i in content_losses:\n",
        "    i.mode = 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nf5Xoal0nN2"
      },
      "source": [
        "### Captura de características del estilo\n",
        "\n",
        "Ahora vamos a repetir lo mismo pero con el array de módulos de calculo de las perdidas en el estilo (cambiar atributos 'mode' de none a capture, ejecutar la red con la imagen de estilo y devolver los modos a su estado original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwhYOQ900nN3"
      },
      "outputs": [],
      "source": [
        "for i in style_losses:\n",
        "    i.mode = 'capture'\n",
        "\n",
        "net(style_image)\n",
        "\n",
        "for i in style_losses:\n",
        "    i.mode = 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYAG8uuC0nN3"
      },
      "source": [
        "### Preparar la red para el calculo de los errores\n",
        "\n",
        "* Vamos a dejar la red preparada para cuando incorporemos el optimizador, para ello, como anteriormente vamos a poner los modos de todos los módulos a pérdida (loss)\n",
        "\n",
        "* Vamos también a congelar los parámetros de la red para evitar los cálculos del gradiente en segundo plano y que ralentizarán bastante el modelo\n",
        "\n",
        "* Por último vamos a pasar nuestro tensor de imagen objetivo a módulo de red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWPRtUfQ0nN3"
      },
      "outputs": [],
      "source": [
        "for i in content_losses:\n",
        "    i.mode = 'loss'\n",
        "for i in style_losses:\n",
        "    i.mode = 'loss'\n",
        "\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "target_image = nn.Parameter(target_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4E8GMeS0nN3"
      },
      "source": [
        "### Optimizador\n",
        "\n",
        "Tenemos que construir un optimizador para tratar de ir cambiando cada iteracion los pesos que se inician aleatoriamente al principio de la ejecucion, de manera que a lo largo de la ejecucion aumentamos la precision de la red\n",
        "\n",
        "‎En cada iteracion, la salida de los datos de entrenamiento se compara con los datos reales con la ayuda de la función de pérdida (o error) para calcular el error y luego el peso se actualiza en consecuencia utilizando dicho optimizador.‎\n",
        "\n",
        "En este caso elegimos el optimizador basado en el algoritmo L-BFGS, utiilizado por defecto en el codigo que implementa el artículo, con los siguientes parámetros\n",
        "\n",
        "* Introducimos la imagen objetivo\n",
        "* Introducimos la tasa de aprendizaje\n",
        "* Introducimos las iteraciones maximas, en este caso las 1000 por defecto que se utilizan en el articulo\n",
        "* Introducimos una tolerancia de cambio, (tolerancia de terminación en‎‎ los cambios de valor/parámetro de la función)\n",
        "* Introducimos una tolerancia de gradiente, (tolerancia de terminación en la optimalidad de primer orden‎)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0fenL3t0nN4"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.LBFGS([target_image], lr = 1, max_iter = 1000, tolerance_change = -1, tolerance_grad = -1)\n",
        "#optimizer = optim.LBFGS([target_image], lr = 0.1, max_iter = 10000, tolerance_change = -1, tolerance_grad = -1)\n",
        "loopVal = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suObSJEq0nN4"
      },
      "source": [
        "### Ejecución del programa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJEK4IBa0nN4"
      },
      "outputs": [],
      "source": [
        "def maybe_save(t):\n",
        "    #should_save = 100 > 0 and t % 100 == 0\n",
        "    #should_save = should_save or t == 1000\n",
        "    should_save = t == 1000\n",
        "    if should_save:\n",
        "        output_filename, file_extension = os.path.splitext('output.jpg')\n",
        "        if t == 1000:\n",
        "            filename = output_filename + str(file_extension)\n",
        "        else:\n",
        "            filename = str(output_filename) + \"_\" + str(t) + str(file_extension)\n",
        "        disp = postprocesamiento(target_image.clone())\n",
        "\n",
        "        disp.save(str(filename))\n",
        "\n",
        "def maybe_show(t):\n",
        "    if t % 100 == 0:\n",
        "        plt.imshow(postprocesamiento(target_image))\n",
        "        plt.show()        \n",
        "\n",
        "num_calls = [0]\n",
        "def closure():\n",
        "    num_calls[0] += 1\n",
        "    optimizer.zero_grad()\n",
        "    net(target_image)\n",
        "    loss = 0\n",
        "\n",
        "    for module in content_losses:\n",
        "        loss += module.loss.to(backward_device)\n",
        "    for module in style_losses:\n",
        "        loss += module.loss.to(backward_device)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    maybe_save(num_calls[0])\n",
        "    maybe_show(num_calls[0])\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "while num_calls[0] <= loopVal:\n",
        "        optimizer.step(closure)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
        "ax1.imshow(postprocesamiento(content_image))\n",
        "ax2.imshow(postprocesamiento(style_image))\n",
        "ax3.imshow(postprocesamiento(target_image))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f7725fad870e428f1e3698a5c0a0de2a7916bab2786fc0bac9119a8ae59d8d68"
    },
    "kernelspec": {
      "display_name": "Python 3.6.13 64-bit ('deep-learning': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Transferencia de estilos CONCURRENCIAS.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}